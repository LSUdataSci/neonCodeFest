---
title: "Baseline forecast and introduction to R"
author: "Tad Dallas"
includes:
  in_header:
    - \usepackage{lmodern}
output:
  pdf_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 6
    toc: yes
  html_document:
    fig_caption: yes
    fig_height: 6
    fig_width: 6
    highlight: tango
    theme: journal
---







## What is R and why should I use it?

R is a statistical programming language that has quickly become the standard language for ecologists, and to some extent, biologists. This does not mean that you have to use it. I hope that many of the skills you learn in this course in terms of how to program are transferable across languages. 



## R basics

### Variables 

Variables are defined as objects in `R` that can take on different attributes. For instance, data you work with could come in the form of `numeric` variables, such as heights, weights, etc. `R` has many different variable types, all of which are defined by their class. A variable `class` defines the attributes of an object, and are generally grouped into 3 different types (S3, S4, and RC). Most of the data you will work with will be S3 or S4, but RC class is the most aligned with `object-oriented programming`. We will focus on base types in this class. Specifically, we will work with four general types of variables: `numeric`, `character`, `factor`, and `logical`. We will use this generic types to build up to "higher order" class structures. For instance, you can define vectors of different lengths which correspond to the 4 different types.

```{r}
num <- c(1,2,3,4)
char <- LETTERS[1:4]
fact <- as.factor(c('a', 'a', 'b', 'd'))
logc <- c(TRUE, FALSE, TRUE, TRUE)

```

We can examine the properties of these vectors by using base `R` functions

```{r}

is.numeric(num)
is.factor(num)

is.factor(fact)
is.logical(fact)

```

These output a boolean (TRUE or FALSE) telling you if the variable is that class.




### Data.frames and subsetting data

The data above (e.g., `num`) were stored as 'vectors', which are simple one-dimensional data structures. Most of the time, we have more than dimension of data. We could imagine wanting the data organized to where each row of the data corresponds to a particular observation, with multiple columns of data describing the observation. For instance, if we considered each row as a state in the USA, columns might correspond to latitude, longitude, land area, state bird, etc. To store this information, we would need a two-dimensional data structure called a `data.frame`.

```{r}

df <- data.frame(state=c('AL', 'GA', 'LA'), 
  latitude=c(32.318, 32.165,30.984),
  longitude=c(86.9023,82.9001,91.962),
  stateBird=c("Northern Flicker", "Brown Thatcher", "Brown pelican"))

head(df)

```








### Reading in data

Reading in and working with data is an essential skill in R. Here, we will go over how to read in the data that you will use to make a predictive forecast.

```{r}

dat <- read.csv('data/neonData.csv')

```













Now we can start to look at properties of the data 

```{r}

head(dat)

```











((further description of the data)) 


```{r}




```










((some pretty plots))




```{r}




```























## What are models?

Models are representations of a system of interest, used for the purposes of inference or prediction. 


> When you create a model of a system, you have two things you don't understand; the system and the model. -- Alan Hastings

But it's still super fun. So let's start developing our first predictive models!













## What are the basic components of a model? 

The response variable should be the thing that you want to predict, or the thing whose variability you want to understand (also sometimes referred to as the dependent variable). Often, it is something that you think is the effect produced by some other cause. For example, in examining the relationship between gas usage and outdoor temperature, it seems clear that gas usage should be the response: temperature is a major determinant of gas usage. But suppose that the modeler wanted to be able to measure outdoor temperature from the amount of gas used. Then it would make sense to take temperature as the response variable.


The explanatory variables are used to explain or predict the response variable (also sometimes referred to as independent variables).




### A simple example

```{r}

y <- runif(100)
x <- y ** runif(100, 1,4)

m1 <- lm(y ~ x)


plot(y=y,x=x, pch=16, 
	ylab='y', xlab='x', 
	col='dodgerblue', las=1)
abline(m1, lwd=2)

```








But to minimize these residuals (or the sum of squares of the residuals), we don't necessarily _need_ a model, right? 



```{r}

par(mar=c(4,4,0.5,0.5))
plot(y=y,x=x, pch=16, 
	ylab='y', xlab='x', 
	col=adjustcolor(1,0.5), las=1)
abline(m1, lwd=2)

lines(smooth.spline(y=y,x=x), col='red', lwd=2)

```





And if we accept that we need a model, what's stopping us from making the model incredibly complex to capture all the possible variation in the data?


```{r}

par(mar=c(4,4,0.5,0.5))
plot(y=y,x=x, pch=16, 
	ylab='y', xlab='x', 
	col=adjustcolor(1,0.5), las=1)
abline(m1, lwd=2)

m3 <- lm(y~poly(x,3))
lines(x=sort(x), fitted(m3)[order(x)], col='green', type='l', lwd=2) 

m4 <- lm(y~poly(x,15))
lines(x=sort(x), fitted(m4)[order(x)], col='purple', type='l', lwd=2) 

```


The model would quickly become uninformative. That is, the model could fit every data point perfectly, but tell us nothing about the general relationship between the two variables. This makes it impossible to use the model to predict. Adding in this level of complexity essentially minimizes the error to the fit, but in turn basically maximizes error when extrapolating to new data. This model would be overfit, and the process of adding in variables that minimize model fit error while making the model useless to other datasets, is called overfitting. Not ideal. 












### Building a baseline model for this challenge

Let's think about the system. Perhaps we can assume that species richness does not change dramatically over the years, but does change seasonally, such that species richness in 2020 could simply be the average species richness for a given season from 2014-2019. That is, species richness in fall of 2020 could be the average richness we observed from the years 2014-2019. 



```{r}

head(dat)

mnRichness <- dat %>% 
  group_by(siteID, season) %>%
  summarise(mnRichness=mean(richness))

```


Now that we have our predictions, we just need to ensure that the format matches that in the `predictionTemplate.csv` file available within this GitHub repository. 


```{r}

preds <- read.csv('predictionTemplate.csv')

preds2 <- dplyr::left_join(preds, mnRichness)
preds2$richness <- NULL
colnames(preds2)[4] <- 'richness'

write.csv(preds2, 'myPredictions.csv')

```














## What do you think influences species richness across the US?


Best of luck in your forecast adventures! 

















